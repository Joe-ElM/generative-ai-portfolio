{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "This project applies Parameter-Efficient Fine-Tuning (PEFT) to a pre-trained language model for a text classification task. The performance of the base model is compared with the PEFT-enhanced model to demonstrate the benefits of this approach.\n",
    "\n",
    "PEFT techniques allow fine-tuning of large pre-trained models with much fewer trainable parameters than traditional full fine-tuning. This makes the process more efficient in terms of computation, memory usage, and storage requirements. LoRA (Low-Rank Adaptation) is one such technique that adds small trainable \"adapter\" modules to the model while keeping most of the original parameters frozen.\n",
    "\n",
    "### Choices for this project:\n",
    "\n",
    "* **PEFT technique**: Low-Rank Adaptation (LoRA) - A popular and effective PEFT method that adds trainable rank decomposition matrices to existing weights.\n",
    "* **Model**: GPT-2 - A versatile language model that can be adapted for classification tasks.\n",
    "* **Evaluation approach**: Accuracy on a test set using the Hugging Face Trainer.\n",
    "* **Fine-tuning dataset**: AG News - A collection of news articles categorized into 4 classes: World, Sports, Business, and Sci/Tech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## 1. Loading Libraries and Dataset\n",
    "\n",
    "This section imports the necessary libraries and loads the AG News dataset. The dataset contains news articles categorized into four classes: World, Sports, Business, and Sci/Tech. This dataset serves as the basis for the text classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from   datasets     import load_dataset\n",
    "from   transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from   transformers import DataCollatorWithPadding\n",
    "from   peft         import LoraConfig, get_peft_model, TaskType, PeftConfig, PeftModel, PeftModelForSequenceClassification\n",
    "from   peft         import AutoPeftModelForSequenceClassification\n",
    "import numpy        as     np\n",
    "import torch\n",
    "import os   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ca6686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for saving models\n",
    "os.makedirs(\"./data/foundation_model_pretrained\", exist_ok=True)\n",
    "os.makedirs(\"./data/foundation_model_finetuned\", exist_ok=True)\n",
    "os.makedirs(\"./data/peft_model_saved\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f46b9982c1c4879b7a87336b3e8820e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 18.6M/18.6M [00:00<00:00, 20.1MB/s]\n",
      "Downloading data: 100%|██████████| 1.23M/1.23M [00:00<00:00, 9.35MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8dc07614694349954d3f7a225fcbf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9a94e1e349494e934971bf99117988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the AG News dataset\n",
    "dataset = load_dataset(\"ag_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33815e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset structure:\n",
      " DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 120000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 7600\n",
      "    })\n",
      "}) \n",
      "\n",
      "# ================================================== \n",
      "\n",
      "Dataset Info:\n",
      "DatasetInfo(description='', citation='', homepage='', license='', features={'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['World', 'Sports', 'Business', 'Sci/Tech'], id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='ag_news', config_name='default', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=29832303, num_examples=120000, shard_lengths=None, dataset_name='ag_news'), 'test': SplitInfo(name='test', num_bytes=1880424, num_examples=7600, shard_lengths=None, dataset_name='ag_news')}, download_checksums={'hf://datasets/ag_news@eb185aade064a813bc0b7f42de02595523103ca4/data/train-00000-of-00001.parquet': {'num_bytes': 18585438, 'checksum': None}, 'hf://datasets/ag_news@eb185aade064a813bc0b7f42de02595523103ca4/data/test-00000-of-00001.parquet': {'num_bytes': 1234829, 'checksum': None}}, download_size=19820267, post_processing_size=None, dataset_size=31712727, size_in_bytes=51532994)\n",
      "Features: {'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['World', 'Sports', 'Business', 'Sci/Tech'], id=None)} \n",
      "\n",
      "Number of training samples: 120000\n",
      "Number of test samples    : 7600\n"
     ]
    }
   ],
   "source": [
    "# Print detailed information about the dataset\n",
    "\n",
    "print(\"The dataset structure:\\n\", dataset, '\\n')\n",
    "print(\"# \"+ \"=\" * 50, '\\n')\n",
    "print(\"Dataset Info:\")\n",
    "print(dataset['train'].info)\n",
    "\n",
    "# Alternatively, print features and other relevant information\n",
    "print(\"Features:\", dataset['train'].features, '\\n')\n",
    "print(\"Number of training samples:\", len(dataset['train']))\n",
    "print(\"Number of test samples    :\", len(dataset['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06254c54",
   "metadata": {},
   "source": [
    "## 2. Creating Smaller Dataset for Faster Training\n",
    "\n",
    "For the purposes of this project, a smaller subset of the AG News dataset is used to make training faster. This smaller dataset demonstrates the effectiveness of PEFT techniques while keeping computational requirements manageable.\n",
    "\n",
    "The implementation randomly samples 1000 training examples and 500 test examples from the original dataset. This is sufficient to show meaningful results while keeping training time reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0055b844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 1250\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 500\n",
       " })}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this cell to generate a smaller shuffeled set of AG News\n",
    "# =======================================================================================\n",
    "\n",
    "# how many lines define\n",
    "train_size = 1250\n",
    "test_size  = 500\n",
    "splits  = [\"train\", \"test\"]\n",
    "dataset = {split: df for split, df in zip(splits, load_dataset(\"ag_news\", split=splits))}\n",
    "\n",
    "# Thin out the dataset to make it run faster for this example\n",
    "dataset['train'] = dataset['train'].shuffle(seed=42).select(range(train_size))      # Shuffle and select 1000 for training\n",
    "dataset['test' ] = dataset['test' ].shuffle(seed=42).select(range(test_size ))      # Shuffle and select 500 for testing\n",
    "\n",
    "# Show the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91281a65",
   "metadata": {},
   "source": [
    "## 3. Mapping Class Labels and Setting up Tokenizer\n",
    "\n",
    "This section creates mappings between class labels and their numeric IDs. This is important for both training and inference, as it allows translation between human-readable categories (like \"World\" or \"Sports\") and the numeric representations used by the model.\n",
    "\n",
    "The code also sets up the tokenizer for GPT-2, which converts text into the numeric format required by the model. Since GPT-2 wasn't designed with a padding token, the end-of-sequence (EOS) token is used as a padding token, which is a common practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc26d446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label2id: {'World': 0, 'Sports': 1, 'Business': 2, 'Sci/Tech': 3}\n",
      "id2label: {0: 'World', 1: 'Sports', 2: 'Business', 3: 'Sci/Tech'}\n"
     ]
    }
   ],
   "source": [
    "# Mapping Class Labels to IDs and Vice Versa\n",
    "# --------------------\n",
    "\n",
    "labels             = dataset['train'].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i]     = label\n",
    "    \n",
    "    \n",
    "print(\"label2id:\", label2id)\n",
    "print(\"id2label:\", id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dc52367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd863f35d5144469054856dc987fff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e05a4e3bf44f859c8c20f808231977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55246a6df79c4a18991c155392b746c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16776927550433589a654735e610dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3183a7781dc64c1fbb5938952ba359b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b33a96539a4422bf74dcf3a115986e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3feee9726aa44f429f706fed481ca220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 1250\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 500\n",
       " })}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the GPT-2 Tokenizer\n",
    "tokenizer           = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#max_length = 128\n",
    "def preprocess_function(News):\n",
    "    \"\"\"\n",
    "        Preprocess the AG News dataset by returning tokenized examples.\n",
    "    \"\"\"\n",
    "    return tokenizer(News['text'], padding=\"max_length\", truncation = True)   #, max_length=128)   #padding=True,\n",
    "\n",
    "tokenized_df = {}\n",
    "for split in  [\"train\", \"test\"]:\n",
    "    tokenized_df[split] = dataset[split].map(preprocess_function, batched = True)\n",
    "    \n",
    "tokenized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f4f144f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43984, 75, 13410, 1582, 47557, 416, 8956, 29560, 7941, 423, 3181, 867, 11684, 290, 4736, 287, 19483, 284, 257, 17369, 11, 262, 1110, 706, 1248, 661, 3724, 287, 23171, 379, 257, 1964, 7903, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
      "Input IDs length: 1024\n"
     ]
    }
   ],
   "source": [
    "# Check a sample of the tokenized data\n",
    "print(tokenized_df['train'][0]['input_ids'][:128])\n",
    "print(\"Input IDs length:\", len(tokenized_df['train'][0]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05efd00d",
   "metadata": {},
   "source": [
    "## 4. Loading and Evaluating the Foundation Model WITHOUT Training\n",
    "\n",
    "This section evaluates the raw pre-trained GPT-2 model on the classification task without any fine-tuning. This provides the true baseline performance of the model.\n",
    "\n",
    "The model is configured for sequence classification with 4 output classes corresponding to the news categories. Since GPT-2 wasn't pre-trained for classification tasks, the `ignore_mismatched_sizes` parameter handles differences in model architecture.\n",
    "\n",
    "This evaluation establishes how well the pre-trained model performs on our specific task before any adaptation is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b45f95ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4475ece3460d46b28edb230aeec3ed20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating foundation model WITHOUT training:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foundation Model Results (before training): {'eval_loss': 4.468132019042969, 'eval_accuracy': 0.214, 'eval_runtime': 41.2338, 'eval_samples_per_second': 12.126, 'eval_steps_per_second': 12.126}\n",
      "Foundation model (pretrained) saved to ./data/foundation_model_pretrained \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.468132019042969,\n",
       " 'eval_accuracy': 0.214,\n",
       " 'eval_runtime': 41.2338,\n",
       " 'eval_samples_per_second': 12.126,\n",
       " 'eval_steps_per_second': 12.126}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define compute_metrics function first\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions         = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "\n",
    "\n",
    "# Load the foundation model for evaluation\n",
    "foundation_model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", \n",
    "                                                                      num_labels              = 4,\n",
    "                                                                      id2label                = id2label,\n",
    "                                                                      label2id                = label2id,\n",
    "                                                                      ignore_mismatched_sizes = True)\n",
    "foundation_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Setup evaluation trainer for foundation model\n",
    "foundation_eval_trainer = Trainer(\n",
    "                                model                      = foundation_model,\n",
    "                                args                       = TrainingArguments(\n",
    "                                output_dir                 = \"./data/foundation_model_pretrained\",\n",
    "                                per_device_eval_batch_size = 1,\n",
    "                                ),\n",
    "    \n",
    "                        eval_dataset    = tokenized_df[\"test\"],\n",
    "                        tokenizer       = tokenizer,\n",
    "                        data_collator   = DataCollatorWithPadding(tokenizer),\n",
    "                        compute_metrics = compute_metrics,\n",
    "                                  )\n",
    "\n",
    "# Evaluate foundation model (no training)\n",
    "print(\"Evaluating foundation model WITHOUT training:\")\n",
    "foundation_model_results = foundation_eval_trainer.evaluate()\n",
    "print(\"Foundation Model Results (before training):\", foundation_model_results)\n",
    "\n",
    "# Save the pretrained foundation model\n",
    "foundation_model.save_pretrained(\"./data/foundation_model_pretrained\")\n",
    "print(\"Foundation model (pretrained) saved to ./data/foundation_model_pretrained\", '\\n')\n",
    "foundation_model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156575d",
   "metadata": {},
   "source": [
    "## 5. Training and Evaluating the Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb0512ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=4, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load a fresh copy of the model for training\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels      = 4,\n",
    "                                                                id2label                = id2label,\n",
    "                                                                label2id                = label2id,\n",
    "                                                                ignore_mismatched_sizes = True) # Needed because GPT-2 wasn't pre-trained for classification\n",
    "\n",
    "\n",
    "for param in base_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(base_model)\n",
    "\n",
    "# Set the pad token in the model config\n",
    "base_model.config.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abbf25ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training base model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 08:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.730200</td>\n",
       "      <td>0.542354</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.547600</td>\n",
       "      <td>0.628192</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.424500</td>\n",
       "      <td>0.566699</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "base_trainer  = Trainer(\n",
    "                        model          = base_model,  # Load the model with 4 labels\n",
    "                        args           = TrainingArguments(\n",
    "                                                        output_dir                  = \"./data/sentiment_analysis\",\n",
    "                                                        learning_rate               = 5e-4, # 2e-3,\n",
    "                                                        per_device_train_batch_size = 1,\n",
    "                                                        per_device_eval_batch_size  = 1,\n",
    "                                                        num_train_epochs            = 3,\n",
    "                                                        weight_decay                = 0.01,\n",
    "                                                        evaluation_strategy         = \"epoch\",\n",
    "                                                        save_strategy               = \"epoch\",\n",
    "                                                        load_best_model_at_end      = True,\n",
    "                                                                ),\n",
    "                        train_dataset   = tokenized_df[\"train\"],\n",
    "                        eval_dataset    = tokenized_df[\"test\"],\n",
    "                        tokenizer       = tokenizer,\n",
    "                        data_collator   = DataCollatorWithPadding(tokenizer),\n",
    "                        compute_metrics = compute_metrics,\n",
    "                        )\n",
    "\n",
    "\n",
    "# Train the base model\n",
    "print(\"Training base model...\")\n",
    "base_training_results = base_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532cd40f",
   "metadata": {},
   "source": [
    "### Evaluate ans save the trained base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1fcc4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating base model AFTER training:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Results (after training): {'eval_loss': 0.5423539876937866, 'eval_accuracy': 0.81, 'eval_runtime': 43.5674, 'eval_samples_per_second': 11.476, 'eval_steps_per_second': 11.476, 'epoch': 3.0}\n",
      "Foundation model (fine-tuned) saved to ./data/foundation_model_finetuned\n"
     ]
    }
   ],
   "source": [
    "# Evaluate trained base model\n",
    "print(\"Evaluating base model AFTER training:\")\n",
    "base_model_results = base_trainer.evaluate()\n",
    "print(\"Base Model Results (after training):\", base_model_results)\n",
    "\n",
    "# Save the fine-tuned base model\n",
    "base_model.save_pretrained(\"./data/foundation_model_finetuned\")\n",
    "print(\"Foundation model (fine-tuned) saved to ./data/foundation_model_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "666b1265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5423539876937866,\n",
       " 'eval_accuracy': 0.81,\n",
       " 'eval_runtime': 43.5674,\n",
       " 'eval_samples_per_second': 11.476,\n",
       " 'eval_steps_per_second': 11.476,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## 6. Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "This section creates a PEFT model using LoRA and fine-tunes it on the same dataset. LoRA works by adding low-rank decomposition matrices to existing weights in the model, which allows adaptation of the model's behavior with very few trainable parameters.\n",
    "\n",
    "The key insight of LoRA is to represent weight updates as a product of two smaller matrices: ΔW = BA, where B and A are low-rank matrices. This dramatically reduces the number of trainable parameters while still allowing the model to adapt to new tasks.\n",
    "\n",
    "Key parameters in the LoRA configuration:\n",
    "- `r=8`: The rank of the low-rank matrices (smaller r means fewer parameters)\n",
    "- `lora_alpha=32`: The scaling factor for the LoRA layers (controls update magnitude)\n",
    "- `lora_dropout=0.1`: Dropout probability for regularization\n",
    "- `target_modules=None`: Apply LoRA to all linear layers in the model\n",
    "- `bias=\"none\"`: Don't train bias parameters\n",
    "- `task_type=\"SEQ_CLS\"`: Configure for sequence classification\n",
    "\n",
    "After setting up the LoRA configuration, the model is converted to a PEFT model and the trainer is initialized with the same parameters used for the base model. This ensures a fair comparison between the two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37c3fb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 817,152 || all params: 125,256,960 || trainable%: 0.6523805144241086\n"
     ]
    }
   ],
   "source": [
    "# Load the base model\n",
    "peft_base_model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", \n",
    "                                                                     num_labels = 4, \n",
    "                                                                     id2label   = id2label,\n",
    "                                                                     label2id   = label2id,\n",
    "                                                                     ignore_mismatched_sizes = True\n",
    "                                                                     )\n",
    "\n",
    "\n",
    "# Set the pad token in the model config\n",
    "peft_base_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Freeze the base model parameters\n",
    "# This is important because LoRA will add adapter modules rather than modify these weights directly\n",
    "# for param in peft_base_model.base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# for param in peft_base_model.transformer.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "    \n",
    "\n",
    "lora_config = LoraConfig(\n",
    "                        r              = 8,\n",
    "                        lora_alpha     = 32,\n",
    "                        lora_dropout   = 0.1,\n",
    "                        target_modules = [\"c_attn\", \"c_proj\"],  # Specifically target attention modules in GPT-2\n",
    "                        bias           = \"none\",\n",
    "                        task_type      = TaskType.SEQ_CLS  # \"SEQ_CLS\" Sequence classification task  # used to be \"SEQ_CLS\"  # Sequence classification task\n",
    "                        )\n",
    "\n",
    "\n",
    "# Convert the model to a PEFT model\n",
    "peft_model = get_peft_model(peft_base_model, lora_config)\n",
    "\n",
    "# peft_model = PeftModelForSequenceClassification(model, lora_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fca2c247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PEFT training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 16:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.760900</td>\n",
       "      <td>1.084642</td>\n",
       "      <td>0.846000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.675900</td>\n",
       "      <td>1.018301</td>\n",
       "      <td>0.864000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.279900</td>\n",
       "      <td>0.961758</td>\n",
       "      <td>0.874000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "peft_trainer  = Trainer(\n",
    "                        model = peft_model,  # Use the PEFT model\n",
    "                        args  = TrainingArguments(\n",
    "                                            output_dir                  = \"./data/lora_sentiment_analysis\",\n",
    "                                            learning_rate               = 5e-4,\n",
    "                                            per_device_train_batch_size = 1,\n",
    "                                            per_device_eval_batch_size  = 1,\n",
    "                                            num_train_epochs            = 3,\n",
    "                                            weight_decay                = 0.01,\n",
    "                                            evaluation_strategy         = \"epoch\",\n",
    "                                            save_strategy               = \"epoch\",\n",
    "                                            load_best_model_at_end      = True,\n",
    "                                                 ),\n",
    "                        train_dataset   = tokenized_df[\"train\"],    # Ensure this contains input_ids\n",
    "                        eval_dataset    = tokenized_df[\"test\"],     # Ensure this contains input_ids\n",
    "                        tokenizer       = tokenizer,\n",
    "                        data_collator   = DataCollatorWithPadding(tokenizer=tokenizer),  # , return_tensors=\"pt\"\n",
    "                        compute_metrics = compute_metrics,\n",
    "                       )\n",
    "\n",
    "print(\"Starting PEFT training...\")\n",
    "peft_training_results = peft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a155e48f",
   "metadata": {},
   "source": [
    "## 7. Evaluating the PEFT Model\n",
    "\n",
    "In this section, the PEFT model is evaluated and its performance is compared to the original model. This comparison helps understand the effectiveness of the LoRA technique for this specific task.\n",
    "\n",
    "The accuracy is measured on the test set and the improvement gained from using PEFT is calculated. This demonstrates whether the parameter-efficient approach can match or exceed the performance of the base model while updating fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d85bde89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating PEFT model:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT Model Results: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9617578983306885,\n",
       " 'eval_accuracy': 0.874,\n",
       " 'eval_runtime': 46.7877,\n",
       " 'eval_samples_per_second': 10.687,\n",
       " 'eval_steps_per_second': 10.687,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate PEFT model\n",
    "print(\"Evaluating PEFT model:\")\n",
    "peft_model_results = peft_trainer.evaluate()\n",
    "print(\"PEFT Model Results:\", \"\\n\")\n",
    "peft_model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304f28e8",
   "metadata": {},
   "source": [
    "### Save the PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab000356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT model saved to ./data/peft_model_saved\n",
      "Tokenizer saved with the model\n"
     ]
    }
   ],
   "source": [
    "# Save the PEFT model\n",
    "peft_model.save_pretrained(\"./data/peft_model_saved\")\n",
    "print(\"PEFT model saved to ./data/peft_model_saved\")\n",
    "\n",
    "# Save the tokenizer alongside the model\n",
    "tokenizer.save_pretrained(\"./data/peft_model_saved\")\n",
    "print(\"Tokenizer saved with the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f94fb5",
   "metadata": {},
   "source": [
    "## 7. Performance Comparison of All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "340aa647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Performance Comparison of All Models -----\n",
      "Foundation Model (before training) Accuracy: 0.2140\n",
      "Foundation Model (after training) Accuracy: 0.8100\n",
      "PEFT Model Accuracy: 0.8740\n",
      "Improvement (PEFT vs. Foundation before training): 0.6600\n",
      "Improvement (PEFT vs. Foundation after training): 0.0640\n"
     ]
    }
   ],
   "source": [
    "# Performance Comparison of All Models\n",
    "print(\"\\n----- Performance Comparison of All Models -----\")\n",
    "print(f\"Foundation Model (before training) Accuracy: {foundation_model_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Foundation Model (after training) Accuracy: {base_model_results['eval_accuracy']:.4f}\")\n",
    "print(f\"PEFT Model Accuracy: {peft_model_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Improvement (PEFT vs. Foundation before training): {peft_model_results['eval_accuracy'] - foundation_model_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Improvement (PEFT vs. Foundation after training): {peft_model_results['eval_accuracy'] - base_model_results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c094335",
   "metadata": {},
   "source": [
    "## 8. Loading the Saved PEFT Model for Inference\n",
    "\n",
    "This section demonstrates how to load the saved PEFT model for inference on new text. This is a crucial step, as it shows how the fine-tuned model can be applied to real-world examples.\n",
    "\n",
    "The sample texts cover different news categories to demonstrate the model's ability to classify various types of news articles. These predictions show that the PEFT-enhanced model can effectively categorize news text into the appropriate classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "841b2f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved PEFT model using the correct class\n",
    "#loaded_peft_model = AutoPeftModelForSequenceClassification.from_pretrained(\"./data/peft_model_saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e765d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# First load the config to get information about the model\n",
    "peft_config = PeftConfig.from_pretrained(\"./data/peft_model_saved\")\n",
    "\n",
    "# Load the PEFT model with the correct number of labels\n",
    "loaded_peft_model = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    \"data/peft_model_saved\",  # Adapter weights\n",
    "    num_labels = 4,  # Ensure this matches your number of classes\n",
    "    id2label   = id2label,\n",
    "    label2id   = label2id\n",
    ")\n",
    "\n",
    "# Create a simple inference function\n",
    "def predict_class(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, \n",
    "                       padding=True, \n",
    "                       truncation=True, \n",
    "                       return_tensors=\"pt\"\n",
    "                      )\n",
    "    \n",
    "    # Get the prediction\n",
    "    with torch.no_grad():\n",
    "        outputs     = loaded_peft_model(**inputs)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "    \n",
    "    # Return the predicted class\n",
    "    return id2label[predictions.item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2d308f",
   "metadata": {},
   "source": [
    "## 9. Performing Inference with a PEFT Model\n",
    "\n",
    "Demonstrating Inference with AutoPeftModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7aa9bbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# First load the config\n",
    "peft_config = PeftConfig.from_pretrained(\"./data/peft_model_saved\")\n",
    "\n",
    "# Load a base model with the correct number of labels\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    num_labels=4,  # Explicitly set to match your saved model\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "base_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Now load the PEFT adapter onto the correctly configured base model\n",
    "from peft import PeftModel\n",
    "loaded_peft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"./data/peft_model_saved\",\n",
    "    is_trainable=False\n",
    ")\n",
    "\n",
    "# Create inference function\n",
    "def predict_class(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, \n",
    "                      padding=True, \n",
    "                      truncation=True, \n",
    "                      return_tensors=\"pt\"\n",
    "                     )\n",
    "    \n",
    "    # Get the prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_peft_model(**inputs)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "    \n",
    "    # Return the predicted class\n",
    "    return id2label[predictions.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ef763ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Predictions:\n",
      "Text: Wall Street edges higher as tech stocks gain\n",
      "Predicted Class: Business\n",
      "\n",
      "Text: Manchester United secure dramatic win in Premier League clash\n",
      "Predicted Class: Sports\n",
      "\n",
      "Text: Scientists discover new exoplanet in habitable zone\n",
      "Predicted Class: Sci/Tech\n",
      "\n",
      "Text: Federal Reserve holds interest rates steady amid economic uncertainty\n",
      "Predicted Class: Business\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model with sample texts\n",
    "sample_texts = [\n",
    "    \"Wall Street edges higher as tech stocks gain\",\n",
    "    \"Manchester United secure dramatic win in Premier League clash\",\n",
    "    \"Scientists discover new exoplanet in habitable zone\",\n",
    "    \"Federal Reserve holds interest rates steady amid economic uncertainty\"\n",
    "]\n",
    "\n",
    "print(\"Model Predictions:\")\n",
    "for text in sample_texts:\n",
    "    predicted_class = predict_class(text)\n",
    "    print(f\"Text: {text}\\nPredicted Class: {predicted_class}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3919b",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "This project successfully implemented Parameter-Efficient Fine-Tuning using LoRA on a GPT-2 model for news classification. The results demonstrate several key findings:\n",
    "\n",
    "1. **Foundation vs Fine-tuned vs PEFT Performance**: \n",
    "   - Foundation Model (no training): 21.4% accuracy\n",
    "   - Fine-tuned Foundation Model: 81% accuracy\n",
    "   - PEFT Model with LoRA: 87.4% accuracy\n",
    "\n",
    "2. **Parameter Efficiency**: As shown by the `print_trainable_parameters()` output, the PEFT approach required training only [P]% of the parameters compared to full fine-tuning.\n",
    "\n",
    "3. **Practical Applicability**: The inference examples demonstrate that the model can effectively categorize different types of news articles into their appropriate classes, with [specific examples from your results].\n",
    "\n",
    "4. **Inference Deployment**: The code demonstrates how to properly load and use a saved PEFT model for inference using `AutoPeftModelForSequenceClassification`, making it ready for real-world applications.\n",
    "\n",
    "These results confirm that PEFT techniques like LoRA offer an efficient way to adapt large pre-trained models to specific tasks while maintaining or improving performance. This approach is particularly valuable when working with limited computational resources or when multiple task-specific adaptations of a model are needed.\n",
    "\n",
    "Future work could explore:\n",
    "- Different PEFT techniques beyond LoRA\n",
    "- Combining multiple PEFT adapters for different tasks\n",
    "- Applying these techniques to larger models where the efficiency gains would be even more significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed41ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
